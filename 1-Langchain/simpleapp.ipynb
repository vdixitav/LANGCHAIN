{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple Genai APP USING LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "#langsmith tracking\n",
    "os.environ['LANGCHAIN_API_KEY']=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data injegion : from the website need to screb the data\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.web_base.WebBaseLoader at 0x2d8cff65f30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader=WebBaseLoader(\"https://python.langchain.com/v0.1/docs/langsmith/walkthrough/\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nLangSmith Walkthrough | ü¶úÔ∏èüîó LangChain\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüõ†Ô∏è LangSmithLangSmith Walkthroughü¶úüï∏Ô∏è LangGraphü¶úÔ∏èüèì LangServeSecurityThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).Ecosystemü¶úüõ†Ô∏è LangSmithLangSmith WalkthroughOn this pageLangSmith WalkthroughLangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will have to iterate on your prompts, chains, and other components to build a high-quality product.LangSmith makes it easy to debug, test, and continuously improve your LLM applications.When might this come in handy? You may find it useful when you want to:Quickly debug a new chain, agent, or set of toolsCreate and manage datasets for fine-tuning, few-shot prompting, and evaluationRun regression tests on your application to confidently developCapture production analytics for product insights and continuous improvementsPrerequisites\\u200bCreate a LangSmith account and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the docsNote LangSmith is in closed beta; we\\'re in the process of rolling it out to more users. However, you can fill out the form on the website for expedited access.Now, let\\'s get started!Log runs to LangSmith\\u200bFirst, configure your environment variables to tell LangChain to log traces. This is done by setting the LANGCHAIN_TRACING_V2 environment variable to true.\\nYou can tell LangChain which project to log to by setting the LANGCHAIN_PROJECT environment variable (if this isn\\'t set, runs will be logged to the default project). This will automatically create the project for you if it doesn\\'t exist. You must also set the LANGCHAIN_ENDPOINT and LANGCHAIN_API_KEY environment variables.For more information on other ways to set up tracing, please reference the LangSmith documentation.NOTE: You can also use a context manager in python to log traces usingfrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    agent.run(\"How many people live in canada as of 2023?\")API Reference:tracing_v2_enabledHowever, in this example, we will use environment variables.%pip install --upgrade --quiet  langchain langsmith langchainhub%pip install --upgrade --quiet  langchain-openai tiktoken pandas duckduckgo-searchimport osfrom uuid import uuid4unique_id = uuid4().hex[0:8]os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"os.environ[\"LANGCHAIN_API_KEY\"] = \"<YOUR-API-KEY>\"  # Update to your API key# Used by the agent in this tutorialos.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"Create the langsmith client to interact with the APIfrom langsmith import Clientclient = Client()Create a LangChain component and log runs to the platform. In this example, we will create a ReAct-style agent with access to a general search tool (DuckDuckGo). The agent\\'s prompt can be viewed in the Hub here.from langchain import hubfrom langchain.agents import AgentExecutorfrom langchain.agents.format_scratchpad.openai_tools import (    format_to_openai_tool_messages,)from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParserfrom langchain_community.tools import DuckDuckGoSearchResultsfrom langchain_openai import ChatOpenAI# Fetches the latest version of this promptprompt = hub.pull(\"wfh/langsmith-agent-prompt:5d466cbc\")llm = ChatOpenAI(    model=\"gpt-3.5-turbo-16k\",    temperature=0,)tools = [    DuckDuckGoSearchResults(        name=\"duck_duck_go\"    ),  # General internet search using DuckDuckGo]llm_with_tools = llm.bind_tools(tools)runnable_agent = (    {        \"input\": lambda x: x[\"input\"],        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(            x[\"intermediate_steps\"]        ),    }    | prompt    | llm_with_tools    | OpenAIToolsAgentOutputParser())agent_executor = AgentExecutor(    agent=runnable_agent, tools=tools, handle_parsing_errors=True)API Reference:AgentExecutorformat_to_openai_tool_messagesOpenAIToolsAgentOutputParserDuckDuckGoSearchResultsChatOpenAIWe are running the agent concurrently on multiple inputs to reduce latency. Runs get logged to LangSmith in the background so execution latency is unaffected.inputs = [    \"What is LangChain?\",    \"What\\'s LangSmith?\",    \"When was Llama-v2 released?\",    \"What is the langsmith cookbook?\",    \"When did langchain first announce the hub?\",]results = agent_executor.batch([{\"input\": x} for x in inputs], return_exceptions=True)results[:2][{\\'input\\': \\'What is LangChain?\\',  \\'output\\': \\'I\\\\\\'m sorry, but I couldn\\\\\\'t find any information about \"LangChain\". Could you please provide more context or clarify your question?\\'}, {\\'input\\': \"What\\'s LangSmith?\",  \\'output\\': \\'I\\\\\\'m sorry, but I couldn\\\\\\'t find any information about \"LangSmith\". It could be a company, a product, or a person. Can you provide more context or details about what you are referring to?\\'}]Assuming you\\'ve successfully set up your environment, your agent traces should show up in the Projects section in the app. Congrats!It looks like the agent isn\\'t effectively using the tools though. Let\\'s evaluate this so we have a baseline.Evaluate Agent\\u200bIn addition to logging runs, LangSmith also allows you to test and evaluate your LLM applications.In this section, you will leverage LangSmith to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps:Create a datasetInitialize a new agent to benchmarkConfigure evaluators to grade an agent\\'s outputRun the agent over the dataset and evaluate the results1. Create a LangSmith dataset\\u200bBelow, we use the LangSmith client to create a dataset from the input questions from above and a list labels. You will use these later to measure performance for a new agent. A dataset is a collection of examples, which are nothing more than input-output pairs you can use as test cases to your application.For more information on datasets, including how to create them from CSVs or other files or how to create them in the platform, please refer to the LangSmith documentation.outputs = [    \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\",    \"LangSmith is a unified platform for debugging, testing, and monitoring language model applications and agents powered by LangChain\",    \"July 18, 2023\",    \"The langsmith cookbook is a github repository containing detailed examples of how to use LangSmith to debug, evaluate, and monitor large language model-powered applications.\",    \"September 5, 2023\",]dataset_name = f\"agent-qa-{unique_id}\"dataset = client.create_dataset(    dataset_name,    description=\"An example dataset of questions over the LangSmith documentation.\",)client.create_examples(    inputs=[{\"input\": query} for query in inputs],    outputs=[{\"output\": answer} for answer in outputs],    dataset_id=dataset.id,)2. Initialize a new agent to benchmark\\u200bLangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn\\'t shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.In this case, we will test an agent that uses OpenAI\\'s function calling endpoints.from langchain import hubfrom langchain.agents import AgentExecutor, AgentType, initialize_agent, load_toolsfrom langchain_openai import ChatOpenAI# Since chains can be stateful (e.g. they can have memory), we provide# a way to initialize a new chain for each row in the dataset. This is done# by passing in a factory function that returns a new chain for each row.def create_agent(prompt, llm_with_tools):    runnable_agent = (        {            \"input\": lambda x: x[\"input\"],            \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(                x[\"intermediate_steps\"]            ),        }        | prompt        | llm_with_tools        | OpenAIToolsAgentOutputParser()    )    return AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True)API Reference:AgentExecutorAgentTypeinitialize_agentload_toolsChatOpenAI3. Configure evaluation\\u200bManually comparing the results of chains in the UI is effective, but it can be time consuming.\\nIt can be helpful to use automated metrics and AI-assisted feedback to evaluate your component\\'s performance.Below, we will create a custom run evaluator that logs a heuristic evaluation.Heuristic evaluatorsfrom langsmith.evaluation import EvaluationResultfrom langsmith.schemas import Example, Rundef check_not_idk(run: Run, example: Example):    \"\"\"Illustration of a custom evaluator.\"\"\"    agent_response = run.outputs[\"output\"]    if \"don\\'t know\" in agent_response or \"not sure\" in agent_response:        score = 0    else:        score = 1    # You can access the dataset labels in example.outputs[key]    # You can also access the model inputs in run.inputs[key]    return EvaluationResult(        key=\"not_uncertain\",        score=score,    )Batch Evaluators\\u200bSome metrics are aggregated over a full \"test\" without being assigned to an individual runs/examples. These could be as simple\\nas common classification metrics like Precision, Recall, or AUC, or it could be another custom aggregate metric.You can define any batch metric on a full test level by defining a function (or any callable) that accepts a list of Runs (system traces) and list of Examples (dataset records).from typing import Listdef max_pred_length(runs: List[Run], examples: List[Example]):    predictions = [len(run.outputs[\"output\"]) for run in runs]    return EvaluationResult(key=\"max_pred_length\", score=max(predictions))Below, we will configure the evaluation with the custom evaluator from above, as well as some pre-implemented run evaluators that do the following:Compare results against ground truth labels.Measure semantic (dis)similarity using embedding distanceEvaluate \\'aspects\\' of the agent\\'s response in a reference-free manner using custom criteriaFor a longer discussion of how to select an appropriate evaluator for your use case and how to create your own\\ncustom evaluators, please refer to the LangSmith documentation.from langchain.evaluation import EvaluatorTypefrom langchain.smith import RunEvalConfigevaluation_config = RunEvalConfig(    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator    evaluators=[        check_not_idk,        # Measures whether a QA response is \"Correct\", based on a reference answer        # You can also select via the raw string \"qa\"        EvaluatorType.QA,        # Measure the embedding distance between the output and the reference answer        # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings())        EvaluatorType.EMBEDDING_DISTANCE,        # Grade whether the output satisfies the stated criteria.        # You can select a default one such as \"helpfulness\" or provide your own.        RunEvalConfig.LabeledCriteria(\"helpfulness\"),        # The LabeledScoreString evaluator outputs a score on a scale from 1-10.        # You can use default criteria or write our own rubric        RunEvalConfig.LabeledScoreString(            {                \"accuracy\": \"\"\"Score 1: The answer is completely unrelated to the reference.Score 3: The answer has minor relevance but does not align with the reference.Score 5: The answer has moderate relevance but contains inaccuracies.Score 7: The answer aligns with the reference but has minor errors or omissions.Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"            },            normalize_by=10,        ),    ],    batch_evaluators=[max_pred_length],)API Reference:EvaluatorTypeRunEvalConfig4. Run the agent and evaluators\\u200bUse the run_on_dataset (or asynchronous arun_on_dataset) function to evaluate your model. This will:Fetch example rows from the specified dataset.Run your agent (or any custom function) on each example.Apply evaluators to the resulting run traces and corresponding reference examples to generate automated feedback.The results will be visible in the LangSmith app.from langchain import hub# We will test this version of the promptprompt = hub.pull(\"wfh/langsmith-agent-prompt:798e7324\")import functoolsfrom langchain.smith import arun_on_dataset, run_on_datasetchain_results = run_on_dataset(    dataset_name=dataset_name,    llm_or_chain_factory=functools.partial(        create_agent, prompt=prompt, llm_with_tools=llm_with_tools    ),    evaluation=evaluation_config,    verbose=True,    client=client,    project_name=f\"tools-agent-test-5d466cbc-{unique_id}\",    # Project metadata communicates the experiment parameters,    # Useful for reviewing the test results    project_metadata={        \"env\": \"testing-notebook\",        \"model\": \"gpt-3.5-turbo\",        \"prompt\": \"5d466cbc\",    },)# Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc.# These are logged as warnings here and captured as errors in the tracing UI.API Reference:arun_on_datasetrun_on_datasetReview the test results\\u200bYou can review the test results tracing UI below by clicking the URL in the output above or navigating to the \"Testing & Datasets\" page in LangSmith  \"agent-qa-{unique_id}\" dataset. This will show the new runs and the feedback logged from the selected evaluators. You can also explore a summary of the results in tabular format below.chain_results.to_dataframe()(Optional) Compare to another prompt\\u200bNow that we have our test run results, we can make changes to our agent and benchmark them. Let\\'s try this again with a different prompt and see the results.candidate_prompt = hub.pull(\"wfh/langsmith-agent-prompt:39f3bbd0\")chain_results = run_on_dataset(    dataset_name=dataset_name,    llm_or_chain_factory=functools.partial(        create_agent, prompt=candidate_prompt, llm_with_tools=llm_with_tools    ),    evaluation=evaluation_config,    verbose=True,    client=client,    project_name=f\"tools-agent-test-39f3bbd0-{unique_id}\",    project_metadata={        \"env\": \"testing-notebook\",        \"model\": \"gpt-3.5-turbo\",        \"prompt\": \"39f3bbd0\",    },)Exporting datasets and runs\\u200bLangSmith lets you export data to common formats such as CSV or JSONL directly in the web app. You can also use the client to fetch runs for further analysis, to store in your own database, or to share with others. Let\\'s fetch the run traces from the evaluation run.Note: It may be a few moments before all the runs are accessible.runs = client.list_runs(project_name=chain_results[\"project_name\"], execution_order=1)# The resulting tests are stored in a project.  You can programmatically# access important metadata from the test, such as the dataset version it was run on# or your application\\'s revision ID.client.read_project(project_name=chain_results[\"project_name\"]).metadata# After some time, the test metrics will be populated as well.client.read_project(project_name=chain_results[\"project_name\"]).feedback_statsConclusion\\u200bCongratulations! You have successfully traced and evaluated an agent using LangSmith!This was a quick guide to get started, but there are many more ways to use LangSmith to speed up your developer flow and produce better results.For more information on how you can get the most out of LangSmith, check out LangSmith documentation, and please reach out with questions, feature requests, or feedback at support@langchain.dev.Help us out by providing feedback on this documentation page:Previousü¶úüõ†Ô∏è LangSmithNextü¶úÔ∏èüèì LangServePrerequisitesLog runs to LangSmithEvaluate Agent1. Create a LangSmith dataset2. Initialize a new agent to benchmark3. Configure evaluation4. Run the agent and evaluatorsReview the test results(Optional) Compare to another promptExporting datasets and runsConclusionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for hug document need to perform chunk\n",
    "\n",
    "## load data--> docs--> divide into chunks-->text--> vectoe embeddings-->store in vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='Skip to main contentThis is documentation for LangChain v0.1, which is no longer actively maintained. Check out the docs for the latest version here.ComponentsIntegrationsGuidesAPI ReferenceMorePeopleVersioningContributingTemplatesCookbooksTutorialsYouTubev0.1Latestv0.2v0.1ü¶úÔ∏èüîóLangSmithLangSmith DocsLangServe GitHubTemplates GitHubTemplates HubLangChain HubJS/TS Docsüí¨SearchGet startedIntroductionQuickstartInstallationUse casesQ&A with RAGExtracting structured outputChatbotsTool use and agentsQuery analysisQ&A over SQL + CSVMoreExpression LanguageGet startedRunnable interfacePrimitivesAdvantages of LCELStreamingAdd message history (memory)MoreEcosystemü¶úüõ†Ô∏è LangSmithü¶úüõ†Ô∏è LangSmithLangSmith Walkthroughü¶úüï∏Ô∏è LangGraphü¶úÔ∏èüèì LangServeSecurityThis is documentation for LangChain v0.1, which is no longer actively maintained.For the current stable version, see this version (Latest).Ecosystemü¶úüõ†Ô∏è LangSmithLangSmith WalkthroughOn this pageLangSmith WalkthroughLangChain makes it easy to prototype LLM'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='actively maintained.For the current stable version, see this version (Latest).Ecosystemü¶úüõ†Ô∏è LangSmithLangSmith WalkthroughOn this pageLangSmith WalkthroughLangChain makes it easy to prototype LLM applications and Agents. However, delivering LLM applications to production can be deceptively difficult. You will have to iterate on your prompts, chains, and other components to build a high-quality product.LangSmith makes it easy to debug, test, and continuously improve your LLM applications.When might this come in handy? You may find it useful when you want to:Quickly debug a new chain, agent, or set of toolsCreate and manage datasets for fine-tuning, few-shot prompting, and evaluationRun regression tests on your application to confidently developCapture production analytics for product insights and continuous improvementsPrerequisites\\u200bCreate a LangSmith account and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the docsNote LangSmith'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content=\"and continuous improvementsPrerequisites\\u200bCreate a LangSmith account and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the docsNote LangSmith is in closed beta; we're in the process of rolling it out to more users. However, you can fill out the form on the website for expedited access.Now, let's get started!Log runs to LangSmith\\u200bFirst, configure your environment variables to tell LangChain to log traces. This is done by setting the LANGCHAIN_TRACING_V2 environment variable to true.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='You can tell LangChain which project to log to by setting the LANGCHAIN_PROJECT environment variable (if this isn\\'t set, runs will be logged to the default project). This will automatically create the project for you if it doesn\\'t exist. You must also set the LANGCHAIN_ENDPOINT and LANGCHAIN_API_KEY environment variables.For more information on other ways to set up tracing, please reference the LangSmith documentation.NOTE: You can also use a context manager in python to log traces usingfrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    agent.run(\"How many people live in canada as of 2023?\")API Reference:tracing_v2_enabledHowever, in this example, we will use environment variables.%pip install --upgrade --quiet  langchain langsmith langchainhub%pip install --upgrade --quiet  langchain-openai tiktoken pandas duckduckgo-searchimport osfrom uuid import uuid4unique_id = uuid4().hex[0:8]os.environ[\"LANGCHAIN_TRACING_V2\"] ='),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='langsmith langchainhub%pip install --upgrade --quiet  langchain-openai tiktoken pandas duckduckgo-searchimport osfrom uuid import uuid4unique_id = uuid4().hex[0:8]os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"os.environ[\"LANGCHAIN_PROJECT\"] = f\"Tracing Walkthrough - {unique_id}\"os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"os.environ[\"LANGCHAIN_API_KEY\"] = \"<YOUR-API-KEY>\"  # Update to your API key# Used by the agent in this tutorialos.environ[\"OPENAI_API_KEY\"] = \"<YOUR-OPENAI-API-KEY>\"Create the langsmith client to interact with the APIfrom langsmith import Clientclient = Client()Create a LangChain component and log runs to the platform. In this example, we will create a ReAct-style agent with access to a general search tool (DuckDuckGo). The agent\\'s prompt can be viewed in the Hub here.from langchain import hubfrom langchain.agents import AgentExecutorfrom langchain.agents.format_scratchpad.openai_tools import (    format_to_openai_tool_messages,)from'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='can be viewed in the Hub here.from langchain import hubfrom langchain.agents import AgentExecutorfrom langchain.agents.format_scratchpad.openai_tools import (    format_to_openai_tool_messages,)from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParserfrom langchain_community.tools import DuckDuckGoSearchResultsfrom langchain_openai import ChatOpenAI# Fetches the latest version of this promptprompt = hub.pull(\"wfh/langsmith-agent-prompt:5d466cbc\")llm = ChatOpenAI(    model=\"gpt-3.5-turbo-16k\",    temperature=0,)tools = [    DuckDuckGoSearchResults(        name=\"duck_duck_go\"    ),  # General internet search using DuckDuckGo]llm_with_tools = llm.bind_tools(tools)runnable_agent = (    {        \"input\": lambda x: x[\"input\"],        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(            x[\"intermediate_steps\"]        ),    }    | prompt    | llm_with_tools    | OpenAIToolsAgentOutputParser())agent_executor = AgentExecutor('),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='lambda x: format_to_openai_tool_messages(            x[\"intermediate_steps\"]        ),    }    | prompt    | llm_with_tools    | OpenAIToolsAgentOutputParser())agent_executor = AgentExecutor(    agent=runnable_agent, tools=tools, handle_parsing_errors=True)API Reference:AgentExecutorformat_to_openai_tool_messagesOpenAIToolsAgentOutputParserDuckDuckGoSearchResultsChatOpenAIWe are running the agent concurrently on multiple inputs to reduce latency. Runs get logged to LangSmith in the background so execution latency is unaffected.inputs = [    \"What is LangChain?\",    \"What\\'s LangSmith?\",    \"When was Llama-v2 released?\",    \"What is the langsmith cookbook?\",    \"When did langchain first announce the hub?\",]results = agent_executor.batch([{\"input\": x} for x in inputs], return_exceptions=True)results[:2][{\\'input\\': \\'What is LangChain?\\',  \\'output\\': \\'I\\\\\\'m sorry, but I couldn\\\\\\'t find any information about \"LangChain\". Could you please provide more context or clarify your question?\\'},'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='\\'What is LangChain?\\',  \\'output\\': \\'I\\\\\\'m sorry, but I couldn\\\\\\'t find any information about \"LangChain\". Could you please provide more context or clarify your question?\\'}, {\\'input\\': \"What\\'s LangSmith?\",  \\'output\\': \\'I\\\\\\'m sorry, but I couldn\\\\\\'t find any information about \"LangSmith\". It could be a company, a product, or a person. Can you provide more context or details about what you are referring to?\\'}]Assuming you\\'ve successfully set up your environment, your agent traces should show up in the Projects section in the app. Congrats!It looks like the agent isn\\'t effectively using the tools though. Let\\'s evaluate this so we have a baseline.Evaluate Agent\\u200bIn addition to logging runs, LangSmith also allows you to test and evaluate your LLM applications.In this section, you will leverage LangSmith to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps:Create a datasetInitialize a new agent to benchmarkConfigure evaluators to grade an agent\\'s'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps:Create a datasetInitialize a new agent to benchmarkConfigure evaluators to grade an agent\\'s outputRun the agent over the dataset and evaluate the results1. Create a LangSmith dataset\\u200bBelow, we use the LangSmith client to create a dataset from the input questions from above and a list labels. You will use these later to measure performance for a new agent. A dataset is a collection of examples, which are nothing more than input-output pairs you can use as test cases to your application.For more information on datasets, including how to create them from CSVs or other files or how to create them in the platform, please refer to the LangSmith documentation.outputs = [    \"LangChain is an open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\",    \"LangSmith is a unified platform for debugging, testing, and'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='open-source framework for building applications using large language models. It is also the name of the company building LangSmith.\",    \"LangSmith is a unified platform for debugging, testing, and monitoring language model applications and agents powered by LangChain\",    \"July 18, 2023\",    \"The langsmith cookbook is a github repository containing detailed examples of how to use LangSmith to debug, evaluate, and monitor large language model-powered applications.\",    \"September 5, 2023\",]dataset_name = f\"agent-qa-{unique_id}\"dataset = client.create_dataset(    dataset_name,    description=\"An example dataset of questions over the LangSmith documentation.\",)client.create_examples(    inputs=[{\"input\": query} for query in inputs],    outputs=[{\"output\": answer} for answer in outputs],    dataset_id=dataset.id,)2. Initialize a new agent to benchmark\\u200bLangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='dataset_id=dataset.id,)2. Initialize a new agent to benchmark\\u200bLangSmith lets you evaluate any LLM, chain, agent, or even a custom function. Conversational agents are stateful (they have memory); to ensure that this state isn\\'t shared between dataset runs, we will pass in a chain_factory (aka a constructor) function to initialize for each call.In this case, we will test an agent that uses OpenAI\\'s function calling endpoints.from langchain import hubfrom langchain.agents import AgentExecutor, AgentType, initialize_agent, load_toolsfrom langchain_openai import ChatOpenAI# Since chains can be stateful (e.g. they can have memory), we provide# a way to initialize a new chain for each row in the dataset. This is done# by passing in a factory function that returns a new chain for each row.def create_agent(prompt, llm_with_tools):    runnable_agent = (        {            \"input\": lambda x: x[\"input\"],            \"agent_scratchpad\": lambda x: format_to_openai_tool_messages('),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='create_agent(prompt, llm_with_tools):    runnable_agent = (        {            \"input\": lambda x: x[\"input\"],            \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(                x[\"intermediate_steps\"]            ),        }        | prompt        | llm_with_tools        | OpenAIToolsAgentOutputParser()    )    return AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True)API Reference:AgentExecutorAgentTypeinitialize_agentload_toolsChatOpenAI3. Configure evaluation\\u200bManually comparing the results of chains in the UI is effective, but it can be time consuming.'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='It can be helpful to use automated metrics and AI-assisted feedback to evaluate your component\\'s performance.Below, we will create a custom run evaluator that logs a heuristic evaluation.Heuristic evaluatorsfrom langsmith.evaluation import EvaluationResultfrom langsmith.schemas import Example, Rundef check_not_idk(run: Run, example: Example):    \"\"\"Illustration of a custom evaluator.\"\"\"    agent_response = run.outputs[\"output\"]    if \"don\\'t know\" in agent_response or \"not sure\" in agent_response:        score = 0    else:        score = 1    # You can access the dataset labels in example.outputs[key]    # You can also access the model inputs in run.inputs[key]    return EvaluationResult(        key=\"not_uncertain\",        score=score,    )Batch Evaluators\\u200bSome metrics are aggregated over a full \"test\" without being assigned to an individual runs/examples. These could be as simple'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='as common classification metrics like Precision, Recall, or AUC, or it could be another custom aggregate metric.You can define any batch metric on a full test level by defining a function (or any callable) that accepts a list of Runs (system traces) and list of Examples (dataset records).from typing import Listdef max_pred_length(runs: List[Run], examples: List[Example]):    predictions = [len(run.outputs[\"output\"]) for run in runs]    return EvaluationResult(key=\"max_pred_length\", score=max(predictions))Below, we will configure the evaluation with the custom evaluator from above, as well as some pre-implemented run evaluators that do the following:Compare results against ground truth labels.Measure semantic (dis)similarity using embedding distanceEvaluate \\'aspects\\' of the agent\\'s response in a reference-free manner using custom criteriaFor a longer discussion of how to select an appropriate evaluator for your use case and how to create your own'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='custom evaluators, please refer to the LangSmith documentation.from langchain.evaluation import EvaluatorTypefrom langchain.smith import RunEvalConfigevaluation_config = RunEvalConfig(    # Evaluators can either be an evaluator type (e.g., \"qa\", \"criteria\", \"embedding_distance\", etc.) or a configuration for that evaluator    evaluators=[        check_not_idk,        # Measures whether a QA response is \"Correct\", based on a reference answer        # You can also select via the raw string \"qa\"        EvaluatorType.QA,        # Measure the embedding distance between the output and the reference answer        # Equivalent to: EvalConfig.EmbeddingDistance(embeddings=OpenAIEmbeddings())        EvaluatorType.EMBEDDING_DISTANCE,        # Grade whether the output satisfies the stated criteria.        # You can select a default one such as \"helpfulness\" or provide your own.        RunEvalConfig.LabeledCriteria(\"helpfulness\"),        # The LabeledScoreString evaluator outputs a score on a scale'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='# You can select a default one such as \"helpfulness\" or provide your own.        RunEvalConfig.LabeledCriteria(\"helpfulness\"),        # The LabeledScoreString evaluator outputs a score on a scale from 1-10.        # You can use default criteria or write our own rubric        RunEvalConfig.LabeledScoreString(            {                \"accuracy\": \"\"\"Score 1: The answer is completely unrelated to the reference.Score 3: The answer has minor relevance but does not align with the reference.Score 5: The answer has moderate relevance but contains inaccuracies.Score 7: The answer aligns with the reference but has minor errors or omissions.Score 10: The answer is completely accurate and aligns perfectly with the reference.\"\"\"            },            normalize_by=10,        ),    ],    batch_evaluators=[max_pred_length],)API Reference:EvaluatorTypeRunEvalConfig4. Run the agent and evaluators\\u200bUse the run_on_dataset (or asynchronous arun_on_dataset) function to evaluate your model. This'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='Reference:EvaluatorTypeRunEvalConfig4. Run the agent and evaluators\\u200bUse the run_on_dataset (or asynchronous arun_on_dataset) function to evaluate your model. This will:Fetch example rows from the specified dataset.Run your agent (or any custom function) on each example.Apply evaluators to the resulting run traces and corresponding reference examples to generate automated feedback.The results will be visible in the LangSmith app.from langchain import hub# We will test this version of the promptprompt = hub.pull(\"wfh/langsmith-agent-prompt:798e7324\")import functoolsfrom langchain.smith import arun_on_dataset, run_on_datasetchain_results = run_on_dataset(    dataset_name=dataset_name,    llm_or_chain_factory=functools.partial(        create_agent, prompt=prompt, llm_with_tools=llm_with_tools    ),    evaluation=evaluation_config,    verbose=True,    client=client,    project_name=f\"tools-agent-test-5d466cbc-{unique_id}\",    # Project metadata communicates the experiment parameters,    #'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='),    evaluation=evaluation_config,    verbose=True,    client=client,    project_name=f\"tools-agent-test-5d466cbc-{unique_id}\",    # Project metadata communicates the experiment parameters,    # Useful for reviewing the test results    project_metadata={        \"env\": \"testing-notebook\",        \"model\": \"gpt-3.5-turbo\",        \"prompt\": \"5d466cbc\",    },)# Sometimes, the agent will error due to parsing issues, incompatible tool inputs, etc.# These are logged as warnings here and captured as errors in the tracing UI.API Reference:arun_on_datasetrun_on_datasetReview the test results\\u200bYou can review the test results tracing UI below by clicking the URL in the output above or navigating to the \"Testing & Datasets\" page in LangSmith  \"agent-qa-{unique_id}\" dataset. This will show the new runs and the feedback logged from the selected evaluators. You can also explore a summary of the results in tabular format below.chain_results.to_dataframe()(Optional) Compare to another prompt\\u200bNow that'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='and the feedback logged from the selected evaluators. You can also explore a summary of the results in tabular format below.chain_results.to_dataframe()(Optional) Compare to another prompt\\u200bNow that we have our test run results, we can make changes to our agent and benchmark them. Let\\'s try this again with a different prompt and see the results.candidate_prompt = hub.pull(\"wfh/langsmith-agent-prompt:39f3bbd0\")chain_results = run_on_dataset(    dataset_name=dataset_name,    llm_or_chain_factory=functools.partial(        create_agent, prompt=candidate_prompt, llm_with_tools=llm_with_tools    ),    evaluation=evaluation_config,    verbose=True,    client=client,    project_name=f\"tools-agent-test-39f3bbd0-{unique_id}\",    project_metadata={        \"env\": \"testing-notebook\",        \"model\": \"gpt-3.5-turbo\",        \"prompt\": \"39f3bbd0\",    },)Exporting datasets and runs\\u200bLangSmith lets you export data to common formats such as CSV or JSONL directly in the web app. You can also use the client'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='\"prompt\": \"39f3bbd0\",    },)Exporting datasets and runs\\u200bLangSmith lets you export data to common formats such as CSV or JSONL directly in the web app. You can also use the client to fetch runs for further analysis, to store in your own database, or to share with others. Let\\'s fetch the run traces from the evaluation run.Note: It may be a few moments before all the runs are accessible.runs = client.list_runs(project_name=chain_results[\"project_name\"], execution_order=1)# The resulting tests are stored in a project.  You can programmatically# access important metadata from the test, such as the dataset version it was run on# or your application\\'s revision ID.client.read_project(project_name=chain_results[\"project_name\"]).metadata# After some time, the test metrics will be populated as well.client.read_project(project_name=chain_results[\"project_name\"]).feedback_statsConclusion\\u200bCongratulations! You have successfully traced and evaluated an agent using LangSmith!This was a quick'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='as well.client.read_project(project_name=chain_results[\"project_name\"]).feedback_statsConclusion\\u200bCongratulations! You have successfully traced and evaluated an agent using LangSmith!This was a quick guide to get started, but there are many more ways to use LangSmith to speed up your developer flow and produce better results.For more information on how you can get the most out of LangSmith, check out LangSmith documentation, and please reach out with questions, feature requests, or feedback at support@langchain.dev.Help us out by providing feedback on this documentation page:Previousü¶úüõ†Ô∏è LangSmithNextü¶úÔ∏èüèì LangServePrerequisitesLog runs to LangSmithEvaluate Agent1. Create a LangSmith dataset2. Initialize a new agent to benchmark3. Configure evaluation4. Run the agent and evaluatorsReview the test results(Optional) Compare to another promptExporting datasets and runsConclusionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings=OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectorstordb=FAISS.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2d8e0158d00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can tell LangChain which project to log to by setting the LANGCHAIN_PROJECT environment variable (if this isn\\'t set, runs will be logged to the default project). This will automatically create the project for you if it doesn\\'t exist. You must also set the LANGCHAIN_ENDPOINT and LANGCHAIN_API_KEY environment variables.For more information on other ways to set up tracing, please reference the LangSmith documentation.NOTE: You can also use a context manager in python to log traces usingfrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    agent.run(\"How many people live in canada as of 2023?\")API Reference:tracing_v2_enabledHowever, in this example, we will use environment variables.%pip install --upgrade --quiet  langchain langsmith langchainhub%pip install --upgrade --quiet  langchain-openai tiktoken pandas duckduckgo-searchimport osfrom uuid import uuid4unique_id = uuid4().hex[0:8]os.environ[\"LANGCHAIN_TRACING_V2\"] ='"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query from vector store db\n",
    "query=\"First, configure your environment variables to tell LangChain to log traces.\"\n",
    "result=vectorstordb.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrival chain ,documents chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "\n",
    "Answer the following questions based only on the provided context:\n",
    "<context>\n",
    "\n",
    "{context}\n",
    "\n",
    "</context>\n",
    "\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n\\nAnswer the following questions based only on the provided context:\\n<context>\\n\\n{context}\\n\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002D8FD51AE30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002D8FD581720>, root_client=<openai.OpenAI object at 0x000002D8FD519AB0>, root_async_client=<openai.AsyncOpenAI object at 0x000002D8FD51B130>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. How do you enable trace logging in LangChain?\n",
      "\n",
      "   To enable trace logging in LangChain, you need to set the `LANGCHAIN_TRACING_V2` environment variable to `true`.\n",
      "\n",
      "2. How can you specify which project to log to in LangChain?\n",
      "\n",
      "   You can specify which project to log to by setting the `LANGCHAIN_PROJECT` environment variable. If this isn't set, runs will be logged to the default project.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Invoke the document chain with input and context\n",
    "result = document_chain.invoke({\n",
    "    \"input\": \"First, configure your environment variables to tell LangChain to log traces.\",\n",
    "    \"context\": [\n",
    "        Document(page_content=\"First, configure your environment variables to tell LangChain to log traces. \"\n",
    "                              \"This is done by setting the LANGCHAIN_TRACING_V2 environment variable to true. \"\n",
    "                              \"You can tell LangChain which project to log to by setting the LANGCHAIN_PROJECT \"\n",
    "                              \"environment variable (if this isn't set, runs will be logged to the default project).\")\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriver=vectorstordb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retriever_chain=create_retrieval_chain(retriver,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002D8E0158D00>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\n\\nAnswer the following questions based only on the provided context:\\n<context>\\n\\n{context}\\n\\n</context>\\n\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x000002D8FD51AE30>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x000002D8FD581720>, root_client=<openai.OpenAI object at 0x000002D8FD519AB0>, root_async_client=<openai.AsyncOpenAI object at 0x000002D8FD51B130>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the resonse from the llm\n",
    "\n",
    "response=retriever_chain.invoke({\"input\":\"First, configure your environment variables to tell LangChain to log traces.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, here are the answers to your potential questions:\\n\\n1. **How do you set up LangChain to log to a specific project?**\\n   - You can set LangChain to log to a specific project by setting the `LANGCHAIN_PROJECT` environment variable. If this variable is not set, logs will default to the project that is automatically created. Additionally, you need to set `LANGCHAIN_ENDPOINT` and `LANGCHAIN_API_KEY` environment variables.\\n\\n2. **How can you log traces using a context manager in Python?**\\n   - You can log traces using a context manager in Python with the following code snippet:\\n     ```python\\n     from langchain_core.tracers.context import tracing_v2_enabled\\n     with tracing_v2_enabled(project_name=\"My Project\"):\\n         agent.run(\"How many people live in Canada as of 2023?\")\\n     ```\\n\\n3. **What is required to start using LangSmith?**\\n   - To start using LangSmith, you need to create a LangSmith account and an API key. LangSmith is in closed beta, but you can request expedited access by filling out a form on their website.\\n\\n4. **What is the purpose of LangSmith?**\\n   - LangSmith is used to log runs, test, and evaluate LLM applications. It provides features for creating benchmark datasets, running AI-assisted evaluators, and tracing agent performance.\\n\\n5. **What environment variable needs to be set to enable tracing in LangChain?**\\n   - The environment variable `LANGCHAIN_TRACING_V2` needs to be set to true to enable tracing in LangChain.\\n\\n6. **How can you provide feedback or get support for LangChain?**\\n   - You can provide feedback or get support by reaching out via email at support@langchain.dev.\\n\\nThese answers are derived from the context provided, and further details can be found in the LangSmith documentation.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='You can tell LangChain which project to log to by setting the LANGCHAIN_PROJECT environment variable (if this isn\\'t set, runs will be logged to the default project). This will automatically create the project for you if it doesn\\'t exist. You must also set the LANGCHAIN_ENDPOINT and LANGCHAIN_API_KEY environment variables.For more information on other ways to set up tracing, please reference the LangSmith documentation.NOTE: You can also use a context manager in python to log traces usingfrom langchain_core.tracers.context import tracing_v2_enabledwith tracing_v2_enabled(project_name=\"My Project\"):    agent.run(\"How many people live in canada as of 2023?\")API Reference:tracing_v2_enabledHowever, in this example, we will use environment variables.%pip install --upgrade --quiet  langchain langsmith langchainhub%pip install --upgrade --quiet  langchain-openai tiktoken pandas duckduckgo-searchimport osfrom uuid import uuid4unique_id = uuid4().hex[0:8]os.environ[\"LANGCHAIN_TRACING_V2\"] ='),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content=\"and continuous improvementsPrerequisites\\u200bCreate a LangSmith account and create an API key (see bottom left corner). Familiarize yourself with the platform by looking through the docsNote LangSmith is in closed beta; we're in the process of rolling it out to more users. However, you can fill out the form on the website for expedited access.Now, let's get started!Log runs to LangSmith\\u200bFirst, configure your environment variables to tell LangChain to log traces. This is done by setting the LANGCHAIN_TRACING_V2 environment variable to true.\"),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='\\'What is LangChain?\\',  \\'output\\': \\'I\\\\\\'m sorry, but I couldn\\\\\\'t find any information about \"LangChain\". Could you please provide more context or clarify your question?\\'}, {\\'input\\': \"What\\'s LangSmith?\",  \\'output\\': \\'I\\\\\\'m sorry, but I couldn\\\\\\'t find any information about \"LangSmith\". It could be a company, a product, or a person. Can you provide more context or details about what you are referring to?\\'}]Assuming you\\'ve successfully set up your environment, your agent traces should show up in the Projects section in the app. Congrats!It looks like the agent isn\\'t effectively using the tools though. Let\\'s evaluate this so we have a baseline.Evaluate Agent\\u200bIn addition to logging runs, LangSmith also allows you to test and evaluate your LLM applications.In this section, you will leverage LangSmith to create a benchmark dataset and run AI-assisted evaluators on an agent. You will do so in a few steps:Create a datasetInitialize a new agent to benchmarkConfigure evaluators to grade an agent\\'s'),\n",
       " Document(metadata={'source': 'https://python.langchain.com/v0.1/docs/langsmith/walkthrough/', 'title': 'LangSmith Walkthrough | ü¶úÔ∏èüîó LangChain', 'description': 'Open In Colab', 'language': 'en'}, page_content='as well.client.read_project(project_name=chain_results[\"project_name\"]).feedback_statsConclusion\\u200bCongratulations! You have successfully traced and evaluated an agent using LangSmith!This was a quick guide to get started, but there are many more ways to use LangSmith to speed up your developer flow and produce better results.For more information on how you can get the most out of LangSmith, check out LangSmith documentation, and please reach out with questions, feature requests, or feedback at support@langchain.dev.Help us out by providing feedback on this documentation page:Previousü¶úüõ†Ô∏è LangSmithNextü¶úÔ∏èüèì LangServePrerequisitesLog runs to LangSmithEvaluate Agent1. Create a LangSmith dataset2. Initialize a new agent to benchmark3. Configure evaluation4. Run the agent and evaluatorsReview the test results(Optional) Compare to another promptExporting datasets and runsConclusionCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogYouTubeCopyright ¬© 2024 LangChain, Inc.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['context']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
